---
title: "model-types"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{model-types}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
# library(transferice)
```

# Data resampling and feature engineering

```{r splits}
library(recipes)
library(rsample)
library(dplyr)

set.seed(2)
# splitting
modern_split <- initial_split(modern, prop = 0.8, strata = "latitude")
# training
modern_train <- training(modern_split)
# testing
modern_test <- testing(modern_split)

# variables
vars <- role_organizer(modern, "t_an")
# taxa
txa <- vars[names(vars) == "predictor"] |> unname()

# recipe
rcp <- recipe(x = modern, vars = vars, roles = names(vars)) |> 
  # rescale predictor
  step_log(dplyr::any_of(txa), offset = 0.025) |> 
  # center predictors
  step_center(dplyr::any_of(txa)) |> 
  # reduce dimensions
  step_pca(dplyr::any_of(txa), num_comp = tune::tune()) |> 
  # remove location that lie close to each other
  step_zerogeodist(lon = longitude, lat = latitude, skip = TRUE) |> 
  # update spatial to predictor
  recipes::update_role(longitude, latitude, new_role = "predictor")
```

# Model specifications

```{r}
library(nlme)
library(parsnip)
library(multilevelmod)

# model
gls_spec <- linear_reg() |> 
  set_engine(
    "gls",  
    control = nlme::lmeControl(opt = 'optim'),
    correlation = nlme::corSpatial(
      form = ~longitude|latitude, 
      type = "g", 
      nugget = TRUE
    )
  )  |> 
  # usage of the model for regression
  set_mode('regression')
```

# Workflow for training

```{r workflow}
library(workflows)
library(tune)
library(yardstick)

# fixed formula
fx <- formula_parser(modern, "t_an", exclude = c("longitude", "latitude"))
  
# workflow
gls_wfl <- workflow() |> 
  add_recipe(rcp) |> 
  add_model(gls_spec, formula = fx) 

# multiple cross validation
set.seed(3)
modern_cv <- vfold_cv(
  training(modern_split), 
  v = 10, 
  strata = "latitude"
)

# tuning grid
tune_grid <- dials::grid_regular(
  tune::extract_parameter_set_dials(gls_wfl), 
  levels = 4
)

# tuning
modern_tune <- tune::tune_grid(
  gls_wfl,
  resamples = modern_cv,
  grid = tune_grid,
  metrics = yardstick::metric_set(yardstick::rmse)
)
```

```{r out}
library(ggplot2)
collect_metrics(modern_tune, summarize = FALSE) |> 
  ggplot(aes(x = num_comp, y = .estimate, group = num_comp)) +
  geom_boxplot()
```

# Select final model

```{r label, options}
# select model with 4 PCs
gls_wfl  <- tune::finalize_workflow(
  gls_wfl ,
  tibble::tibble(num_comp = 4)
)

# metrics to return
mts <- yardstick::metric_set(
  yardstick::rmse
)

# fit the final model (does not need to be tuned)
final <- tune::last_fit(gls_wfl, split = modern_split, metrics = mts)
```

```{r check}
ggpartial(final, gls_wfl, pred = "taxa_2", out = "t_an", type = "bubble")
```

# Predictions

```{r predict, options}
# finalized workflow fitted on all data
final_fit <- fit(gls_wfl, data = modern)

# new recipe
# variables
vars <- role_organizer(fossil, NULL)
# taxa
txa <- vars[names(vars) == "predictor"] |> unname()

# recipe
rcp <- recipe(x = fossil, vars = vars, roles = names(vars)) |> 
  # rescale predictor
  step_log(dplyr::any_of(txa), offset = 0.025) |> 
  # center predictors
  step_center(dplyr::any_of(txa)) |> 
  # reduce dimensions (with 4 dims)
  step_pca(dplyr::any_of(txa), num_comp = 4) 

# extract model
final_mdl <- final_fit |> extract_fit_parsnip()

# bake fossil data
fossil_prepped <- prep(rcp, training = fossil) |> 
    bake(new_data = NULL)

# predict on new data
pred <- predict(final_mdl, new_data = fossil_prepped)

# add to original data
fossil_pred <- bind_cols(fossil, pred)

# timeplot
ggplot(fossil_pred, aes(x = age_ma, y = .pred)) +
  geom_point() +
  geom_line() +
  scale_x_reverse() +
  ylab(oceanexplorer::env_parm_labeller("t"))
```


