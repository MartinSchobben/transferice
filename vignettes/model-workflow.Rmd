---
title: "Model workflow for transfer functions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model workflow for transfer functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(transferice)
```

For predictive modelling it is necessary to obtain a model that can generalize patterns in the data. The fundamental process to achieve this is data splitting, By leaving out a portion of the data during model construction, this provides an empirical way to evaluate a model's performance outside the scope of the original data.

To achieve this this package use the tidymodels metapackage, where separate packages deal with re-sampling, model fitting and evaluation.

# Data

To exemplify the package goal of constructing transfer function for paleo-environmental reconstructions, we use a dataset containing global dinocyst from the surface sediment layer. For this data we want to evaluate their predictive value for oceanographic parameters (e.g., temperature, salinity, and nutrient concentrations) at a 30 meter water depth - the approximate habitat of dinoflagellates (which produce the resting stage cysts: "dinocysts"). Besides the different species and environmental parameters contains the coordinates (longitude and latitude) and metadata of the location (hole_id and sample_id).

```{r data}
str(dinodat[1:10,1:15])
```

The basic model constitutes thus a multivariate multiple regression model of matrix with $n$ dinocyst species and $m$ sites ($\mathbb{X}$) and a matrix of $n$ oceanographic parameters and again $m$ sites ($\mathbb{Y}$).

$$\mathbb{Y}= \mathbb{X} \mathbf{B} + \mathbf{E} $$


# Splitting data with re-sampling

A model that generalize well for a dataset should guard against model overfitting. Model overfitting means that a model might be well-fitted to the original data, but not necessarily fits an unknown data for future predictions. Hence, we split the data into a training set for model constructions, and a testing set model to assess predicitive performance outside the bounds of the original data. This is done with *rsample* [@rsample], like so:

```{r splits}
# install.packages("rsample")
library(rsample)
# splitting
dinodat_split <- initial_split(dinodat, prop = 0.75) 
# training
dinodat_train <- training(dinodat_split)
# testing
dinodat_test <- testing(dinodat_split)
```


# Model construction with parsnip

Due to standardaization interfaces with tidymodels *parsnip* the search of suitable transfer functuinsis simplified. The method to solve the linear regression as specified above can be easily altered from ordinary elast square regression to one of a regularized form, such as a Bayesian approach with *rstanarm*. 

```{r label, options}
# install.packages("parsnip")
library(parsnip)

# set type of model (linear regression)
lm_model <- linear_reg() |> 
  # set the engine (ordinary least squares with base lm)
  set_engine('lm') |> 
  # usage of the model for regression
  set_mode('regression')

# set type of model (linear regression)
stan_model <- linear_reg() |> 
  # set the engine (bayesian with rstanarm)
  set_engine('stan_glmer') |> 
  # usage of the model for regression
  set_mode('regression')
```

# Feature engineering with recipes

```{r recipe}
# install.packages("recipes")
library(recipes)

# parse formula
fml <- formula_parser(dinodat, parms)

# formula for recipes
dinodat_recipe <- recipe(fml, data = dinodat) |> 
  # make dino count log odds
  step_logit(all_predictors(), offset = 0.025) |>  
  # reduce multi-dimensionality
  step_pca(all_predictors(), options = list(center =  TRUE)) 
```

Prepare and apply to training set

```{r preptrain}
# seed for reproducibility
set.seed(114393746)

dinodat_train_prep <- prep(dinodat_recipe, training = dinodat_train) |> 
  bake(new_data = NULL)
```

Prepare and apply to testing set

```{r preptest}
dinodat_test_prep <- prep(dinodat_recipe, training = dinodat_train) |> 
  bake(new_data = dinodat_test)
```

# Fit model and test

```{r fit}
# need to rewrite formula as parsnip does not accept multivariate model
fml2 <- formula_parser(dinodat, parms, type = "tidymodels")
fml2 <- as.formula("t_an + p_an + n_an + i_an + o_an + s_an + I_an ~.")

# fit (OLS)
lm_fit <- lm_model %>%   
  fit(fml2,  data = dinodat_train_prep)
# fit (stan)
stan_fit <- stan_model |>    
  fit(fml2,  data = dinodat_train_prep)
```


```{r predict}
dinodat_predict_lm <- predict(lm_fit, new_data = dinodat_test_prep)

# bind test
dinodat_test_prep_lm <- rename_with(
  dinodat_test_prep, 
  .cols = ends_with("_an"), 
  .fn = ~paste0(".truth_", .x)
)
outcome_lm <- bind_cols(dinodat_test_prep_lm, dinodat_predict_lm)

dinodat_predict_stan <- predict(stan_fit, new_data = dinodat_test_prep)

# bind test
dinodat_test_prep_stan <- rename_with(
  dinodat_test_prep, 
   .cols = ends_with("_an"), 
  .fn = ~paste0(".truth_", .x)
)
outcome_stan <- bind_cols(dinodat_test_prep_stan, dinodat_predict_stan)
```

# Evaluate model with yardstick

```{r metrics}
# install.packages("yardstick")
library(yardstick)
# metrics
rmsre(
  outcome, 
  truth =  rlang::inject(c(!!! rlang::syms(paste0(".truth_", sl_parms)))), 
  estimate = rlang::inject(c(!!! rlang::syms(paste0(".pred_", sl_parms))))
)
```

visualize fit

```{r viz}
# install.packages("ggplot2")
# install.packages("tune")
library(ggplot2)
library(tidyr)
library(tune)
long_outcome <- pivot_longer(
  outcome,
  cols = !c(PC1, PC2, PC3, PC4, PC5),
  names_to = c(".value", "parameter"),
  names_pattern = "(.*)_(._an)"
)

ggplot(long_outcome, aes(x = .truth, y = .pred)) +  
  geom_point() +  
  geom_abline(color = 'blue', linetype = 2) +  
  # coord_fixed() +  
  facet_wrap(vars(parameter), scales = "free") +
  labs(
    title = 'R-Squared Plot',       
    y = 'Predicted',        
    x = 'Actual'
  )
```

How does this look like in a spatial sense?

```{r spatpat}
library(sf)
library(tibble)
library(gstat)
# make coordinate sf object
tot <- add_column(
  outcome, 
  longitude = dinodat_test$longitude, 
  latitude = dinodat_test$latitude
)
crds <- st_as_sf(tot, coords = c("longitude", "latitude"), crs =  4326)

# get nicely spread POINT with no duplication within tolerance limit
d = st_is_within_distance(crds, dist = 2500)
dupl = unlist(mapply(function(x,y) x[x < y], d, seq_along(d)))
crds <- crds[-dupl, ]

# variogram (exploration)
v <- variogram(.pred_t_an ~ .truth_t_an, crds) 
plot(v)
# variogram (modelling)
# mean variance
mean_var <- yardstick::rmse(tot, .truth_t_an, .pred_t_an)$.estimate 
vario_model <- fit.variogram(v, vgm(var(tot$.pred_t_an), "Sph", 700, mean_var)) 

plot(v, vario_model)

g <- gstat(formula = .pred_t_an ~ .truth_t_an, data = crds, model = vario_model)

base <- oceanexplorer::get_NOAA("temperature", 1, "annual") |> 
  oceanexplorer::filter_NOAA(depth = 0) |> 
  stars::st_warp(crs = 4326) |>  
  rename(.truth_t_an = "t_an")

z <- predict(g, base)

z = z["var1.pred",,]
names(z) = "t_an"
oceanexplorer::plot_NOAA(z, points = crds)
```

# Compact format with workflows and automated testing

```{r workflows}
# install.packages("workflows")
library(workflows)
lm_wfl <- workflow() |> 
  add_recipe(dinodat_recipe) |> 
  add_model(lm_model)

outcome <- last_fit(lm_wfl, split = dinodat_split, metrics = metric_set(rmsre, rmse))
collect_metrics(outcome)
```

# Tuning the model with cross validation

Now we want to know what actually would be the optimal selection of components from the PCA.

```{r tune}
# install.packages("dials")
library(dials)
dinodat_recipe_tune <- recipe(fml, data = dinodat) |> 
  step_logit(all_predictors(), offset = 0.025) |> 
  # step_scale(all_outcomes()) |> 
  step_pca(all_predictors(), num_comp = tune(), options = list(center =  TRUE))

# update workflow
(lm_tune_wfl <- update_recipe(lm_wfl, dinodat_recipe_tune))
```

cross validation

```{r cv}
# seed for reproducibility
set.seed(410174750)

dinodat_cv <- vfold_cv(dinodat_train, v = 10)
```

grid search

```{r grid, cache=TRUE}
# update dial  (original is only 4 comps) (take 20)
library(hardhat)
lm_tune_parms <- lm_tune_wfl |>
  parameters() |>
  update(num_comp = num_comp(c(1, 9)))

lm_tune_parms|> hardhat::extract_parameter_dials("num_comp")

lm_grid <- grid_regular(lm_tune_parms, levels = 9)

# seed for reproducibility
set.seed(949004590)

dinodat_tuned <- tune_grid(
  lm_tune_wfl, 
  resamples = dinodat_cv, 
  grid = lm_grid, 
  metrics = metric_set(rmsre), 
  control = control_resamples(extract = function(x) extract_fit_parsnip(x))
  )
```

get the metrics

```{r iterate}
collect_metrics(dinodat_tuned)
collect_metrics(dinodat_tuned, summarize = FALSE) |> 
  filter(.estimate < 100) |>
  ggplot(aes(x = num_comp, y = .estimate, group = num_comp)) +
  geom_boxplot()
```

# Partial regression plots

```{r partial}
mdl_extracts <- cv_model_extraction(dinodat_tuned) 
ggpartial(mdl_extracts, 1, "t_an", plot_type = "static")
```

# Finalize

```{r final}
# select best
lm_final_wfl <- finalize_workflow(
  lm_tune_wfl, 
  tibble(num_comp = 3)#, .config = "Preprocessor03_Model1")
)
final_outcome <- last_fit(
  lm_final_wfl, 
  split = dinodat_split,
  metrics = metric_set(rmsre, rmse, rsq)
)
# metrics
final_outcome |> collect_metrics()

final_fit <- rename_with(
  collect_predictions(final_outcome), 
  .cols = any_of(sl_parms), 
  .fn = ~paste0(".truth_", .x)
)

long_final <- pivot_longer(
  final_fit,
  cols = ends_with("_an"),
  names_to = c(".value", "parameter"),
  names_pattern = "(.*)_(._an)"
)

ggplot(long_final, aes(x = .truth, y = .pred)) +  
  geom_point() +  
  geom_abline(color = 'blue', linetype = 2) +  
  # coord_fixed() +  
  facet_wrap(vars(parameter), scales = "free") +
  labs(
    title = 'R-Squared Plot',       
    y = 'Predicted',        
    x = 'Actual'
)
```
