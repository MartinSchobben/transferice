---
title: "Model workflow for transfer functions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model workflow for transfer functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(transferice)
```

For predictive modelling it is necessary to obtain a model that can generalize patterns in the data. The fundamental process to achieve this is data splitting, By leaving out a portion of the data during model construction, this provides an empirical way to evaluate a model's performance outside the scope of the original data.

To achieve this this package use the tidymodels metapackage, where separate packages deal with re-sampling, model fitting and evaluation.

# Data

To exemplify the package goal of constructing transfer function for paleo-environmental reconstructions, we use a dataset containing global dinocyst from the surface sediment layer. For this data we want to evaluate their predictive value for oceanographic parameters (e.g., temperature, salinity, and nutrient concentrations) at a 30 meter water depth - the approximate habitat of dinoflagellates (which produce the resting stage cysts: "dinocysts"). Besides the different species and environmental parameters contains the coordinates (longitude and latitude) and metadata of the location (hole_id and sample_id).

```{r data}
str(dinodat[1:10,1:15])
```

The basic model constitutes thus a multivariate multiple regression model of matrix with $n$ dinocyst species and $m$ sites ($\mathbb{X}$) and a matrix of $n$ oceanographic parameters and again $m$ sites ($\mathbb{Y}$).

$$\mathbb{Y}= \mathbb{X} \mathbf{B} + \mathbf{E} $$


# Splitting data with re-sampling

A model that generalize well for a dataset should guard against model overfitting. Model overfitting means that a model might be well-fitted to the original data, but not necessarily fits an unknown data for future predictions. Hence, we split the data into a training set for model constructions, and a testing set model to assess predicitive performance outside the bounds of the original data. This is done with *rsample* [@rsample], like so:

```{r splits}
# install.packages("rsample")
library(rsample)
# splitting
dinodat_split <- initial_split(dinodat, prop = 0.75) 
# training
dinodat_train <- training(dinodat_split)
# testing
dinodat_test <- testing(dinodat_split)
```


# Model construction with parsnip

Due to standardaization interfaces with tidymodels *parsnip* the search of suitable transfer functuinsis simplified. The method to solve the linear regression as specified above can be easily altered from ordinary elast square regression to one of a regularized form, such as a Bayesian approach with *rstanarm*. 

```{r label, options}
# install.packages("parsnip")
library(parsnip)

# set type of model (linear regression)
lm_model <- linear_reg() |> 
  # set the engine (ordinary least squares with base lm)
  set_engine('lm') |> 
  # usage of the model for regression
  set_mode('regression')

```

# Feature engineering with recipes

```{r recipe}
# install.packages("recipes")
library(recipes)

# variables
vars <- role_organizer(dinodat, "t_an")
# taxa
txa <- vars[names(vars) == "predictor"]

# formula for recipes
dinodat_recipe <- recipe(x = dinodat, vars = vars, roles = names(vars)) |> 
  # scale all outcomes
  recipes::step_normalize(recipes::all_outcomes()) |> 
  # rename taxa
  recipes::step_rename(!!!rlang::set_names(txa, paste0("taxa_", seq_along(txa)))) |> 
  # make dino count log odds
  step_logit(dplyr::any_of(txa), offset = 0.025) |>  
  # center predictors
  step_center(dplyr::any_of(txa)) |> 
  # reduce multi-dimensionality
  step_pca(dplyr::any_of(txa)) 

```

Prepare and apply to training set

```{r preptrain}
# seed for reproducibility
set.seed(114393746)

dinodat_train_prep <- prep(dinodat_recipe, training = dinodat_train) |> 
  bake(new_data = NULL)
```

Prepare and apply to testing set

```{r preptest}
dinodat_test_prep <- prep(dinodat_recipe, training = dinodat_train) |> 
  bake(new_data = dinodat_test)
```

# Fit model and test

```{r fit}
# need to rewrite formula as parsnip does not accept multivariate model
fml <- formula_parser(dinodat, "t_an", aliases = "taxa_")

# fit (OLS)
lm_fit <- lm_model %>%   
  fit(fml,  data = dinodat_train_prep)
```


```{r predict}
# predicting
dinodat_predict_lm <- predict(lm_fit, new_data = dinodat_test_prep)

# results
outcome <- bind_cols(dinodat_test_prep, dinodat_predict_lm)
```

# Evaluate model with yardstick


visualize fit

```{r viz}
library(ggplot2)

ggplot(outcome, aes(x = t_an, y = .pred)) +  
  geom_point() +  
  geom_abline(color = 'blue', linetype = 2) +  
  labs(
    title = 'R-Squared Plot',       
    y = 'Predicted',        
    x = 'Actual'
  )
```


# Compact format with workflows and automated testing

```{r workflows}
# install.packages("workflows")
library(tune)
library(workflows)
library(yardstick)

# workflow for regression analysis with reciped
lm_wfl <- workflow() |> 
  add_recipe(dinodat_recipe) |> 
  add_model(lm_model)

outcome <- last_fit(lm_wfl, split = dinodat_split, metrics = metric_set(rmse))
collect_metrics(outcome)
```

# Tuning the model with cross validation

Now we want to know what actually would be the optimal selection of components from the PCA.

```{r tune}
# install.packages("dials")
library(dials)
# formula for recipes
dinodat_recipe_tune <- recipe(x = dinodat, vars = vars, roles = names(vars)) |> 
  # scale all outcomes
  recipes::step_normalize(recipes::all_outcomes()) |> 
  # rename taxa
  recipes::step_rename(!!!rlang::set_names(txa, paste0("taxa_", seq_along(txa)))) |> 
  # make dino count log odds
  step_logit(dplyr::any_of(paste0("taxa_", seq_along(txa))), offset = 0.025) |>  
  # center predictors
  step_center(dplyr::any_of(paste0("taxa_", seq_along(txa)))) |> 
  # reduce multi-dimensionality
  step_pca(dplyr::any_of(paste0("taxa_", seq_along(txa))), num_comp = tune()) 

# update workflow
(lm_tune_wfl <- update_recipe(lm_wfl, dinodat_recipe_tune))
```

cross validation

```{r cv}
# seed for reproducibility
set.seed(410174750)

dinodat_cv <- vfold_cv(dinodat_train, v = 10)
```

grid search

```{r grid, cache=TRUE}
# search frid
lm_grid <- grid_regular(extract_parameter_set_dials(lm_tune_wfl), levels = 4)

# seed for reproducibility
set.seed(949004590)

dinodat_tuned <- tune_grid(
  lm_tune_wfl, 
  resamples = dinodat_cv, 
  grid = lm_grid, 
  metrics = metric_set(rmse), 
  control = ctrl
)
```

get the metrics

```{r iterate}
collect_metrics(dinodat_tuned)
collect_metrics(dinodat_tuned, summarize = FALSE) |> 
  filter(.estimate < 100) |>
  ggplot(aes(x = num_comp, y = .estimate, group = num_comp)) +
  geom_boxplot()
```

# Partial regression plots

```{r partial}
ggpartial(dinodat_tuned, lm_tune_wfl, tune = 1, out = "t_an", id = "test", plot_type = "static")
```

# Finalize

```{r final}
# select best
lm_final_wfl <- finalize_workflow(
  lm_tune_wfl, 
  tibble(num_comp = 3)
)
final_outcome <- last_fit(
  lm_final_wfl, 
  split = dinodat_split,
  metrics = metric_set(rmsre, rmse, rsq)
)
# metrics
final_outcome |> collect_metrics()

final_fit <- rename_with(
  collect_predictions(final_outcome), 
  .cols = any_of(pms), 
  .fn = ~paste0(".truth_", .x)
)

long_final <- pivot_longer(
  final_fit,
  cols = ends_with("_an"),
  names_to = c(".value", "parameter"),
  names_pattern = "(.*)_(._an)"
)

ggplot(long_final, aes(x = .truth, y = .pred)) +  
  geom_point() +  
  geom_abline(color = 'blue', linetype = 2) +  
  # coord_fixed() +  
  facet_wrap(vars(parameter), scales = "free") +
  labs(
    title = 'R-Squared Plot',       
    y = 'Predicted',        
    x = 'Actual'
)
```
